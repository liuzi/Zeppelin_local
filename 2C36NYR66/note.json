{
  "paragraphs": [
    {
      "text": "/**\n * Simpliest tutorial\n **/ \nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.Duration\nimport org.apache.spark.streaming.Seconds\n\nval ssc \u003d new StreamingContext(sc, Seconds(1))\nval lines \u003d ssc.socketTextStream(\"localhost\",7777)\nval errorLines \u003d lines.filter(_.contains(\"error\"))\nerrorLines.print()\n\nssc.start()\nssc.awaitTermination()",
      "dateUpdated": "Dec 1, 2016 3:11:58 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1477897503248_1712688644",
      "id": "20161031-150503_441367746",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\ndefined class Record\n\nrdd: org.apache.spark.rdd.RDD[Record] \u003d ParallelCollectionRDD[0] at parallelize at \u003cconsole\u003e:29\n\n\n\n\u003cconsole\u003e:32: error: value registerTempTable is not a member of org.apache.spark.rdd.RDD[Record]\n       rdd.registerTempTable(\"records\")\n           ^\n"
      },
      "dateCreated": "Oct 31, 2016 3:05:03 AM",
      "dateStarted": "Oct 31, 2016 3:07:39 AM",
      "dateFinished": "Oct 31, 2016 3:07:42 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%dep\nz.load(\"org.apache.spark:spark-streaming-twitter_2.10:1.5.1\")\nz.load(\"org.apache.spark:spark-streaming_2.10:1.6.2\")",
      "dateUpdated": "Dec 1, 2016 3:42:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1477897579939_-1273439515",
      "id": "20161031-150619_1409997486",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "DepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency \u003d org.apache.zeppelin.dep.Dependency@7f7de8f2\n"
      },
      "dateCreated": "Oct 31, 2016 3:06:19 AM",
      "dateStarted": "Dec 1, 2016 3:13:09 AM",
      "dateFinished": "Dec 1, 2016 3:13:17 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "/**\n * Zeppelin\u0027s tutorial for streaming data from tweets\n **/\n\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter._\nimport org.apache.spark.storage.StorageLevel\nimport scala.io.Source\nimport java.io.File\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport sys.process.stringSeqToProcess\n\n/** Configures the Oauth Credentials for accessing Twitter */\ndef configureTwitterCredentials(apiKey: String, apiSecret: String, accessToken: String, accessTokenSecret: String) {\n    import scala.collection.mutable.HashMap\n  val configs \u003d new HashMap[String, String] ++\u003d Seq(\n    \"apiKey\" -\u003e apiKey, \"apiSecret\" -\u003e apiSecret, \"accessToken\" -\u003e accessToken, \"accessTokenSecret\" -\u003e accessTokenSecret)\n  println(\"Configuring Twitter OAuth\")\n  configs.foreach{ case(key, value) \u003d\u003e\n    if (value.trim.isEmpty) {\n      throw new Exception(\"Error setting authentication - value for \" + key + \" not set\")\n    }\n    val fullKey \u003d \"twitter4j.oauth.\" + key.replace(\"api\", \"consumer\")\n    System.setProperty(fullKey, value.trim)\n    println(\"\\tProperty \" + fullKey + \" set as [\" + value.trim + \"]\")\n  }\n  println()\n}\n\n// Configure Twitter credentials\nval apiKey \u003d \"R4rKoIi7xchWJFGop9pH5WMF7\"\nval apiSecret \u003d \"nJ6mv3FtzswiXUUMvA9bCQLWZ2JWn4019y4axan751i32uHfup\"\nval accessToken \u003d \"801968074819653632-JruLx4Tz7JxpGkBN0EkJE7CJEmGJ4tm\"\nval accessTokenSecret \u003d \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\nconfigureTwitterCredentials(apiKey, apiSecret, accessToken, accessTokenSecret)\n\nimport org.apache.spark.streaming.twitter._\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.Duration\nimport org.apache.spark.streaming.Seconds\nimport sqlContext.implicits._\n\nval ssc \u003d new StreamingContext(sc, Seconds(2))\nval tweets \u003d TwitterUtils.createStream(ssc, None)\nval twt \u003d tweets.window(Seconds(60))\n\ncase class Tweet(createdAt:Long, text:String)\ntwt.map(status\u003d\u003e\n  Tweet(status.getCreatedAt().getTime()/1000, status.getText())\n).foreachRDD(rdd\u003d\u003e\n  // Below line works only in spark 1.3.0.\n  // For spark 1.1.x and spark 1.2.x,\n  // use rdd.registerTempTable(\"tweets\") instead.\n  rdd.toDF().registerTempTable(\"tweets\")\n)\n\ntwt.print\n\nssc.start()",
      "dateUpdated": "Dec 1, 2016 3:20:30 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1480576287448_-1251937075",
      "id": "20161201-151127_2015775519",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.streaming._\n\nimport org.apache.spark.streaming.twitter._\n\nimport org.apache.spark.storage.StorageLevel\n\nimport scala.io.Source\n\nimport java.io.File\n\nimport org.apache.log4j.Logger\n\nimport org.apache.log4j.Level\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\nConfiguring Twitter OAuth\n\tProperty twitter4j.oauth.consumerKey set as [R4rKoIi7xchWJFGop9pH5WMF7]\n\tProperty twitter4j.oauth.accessToken set as [801968074819653632-JruLx4Tz7JxpGkBN0EkJE7CJEmGJ4tm]\n\tProperty twitter4j.oauth.consumerSecret set as [nJ6mv3FtzswiXUUMvA9bCQLWZ2JWn4019y4axan751i32uHfup]\n\tProperty twitter4j.oauth.accessTokenSecret set as [xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njava.lang.NoClassDefFoundError: org/apache/spark/Logging\n  at java.lang.ClassLoader.defineClass1(Native Method)\n  at java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n  at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n  at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n  at org.apache.spark.streaming.twitter.TwitterUtils$.createStream(TwitterUtils.scala:44)\n  ... 40 elided\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.Logging\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n  ... 52 more\n"
      },
      "dateCreated": "Dec 1, 2016 3:11:27 AM",
      "dateStarted": "Dec 1, 2016 3:20:30 AM",
      "dateFinished": "Dec 1, 2016 3:20:33 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Dec 1, 2016 3:11:39 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1480576289889_-725985328",
      "id": "20161201-151129_557624813",
      "dateCreated": "Dec 1, 2016 3:11:29 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "spark learning",
  "id": "2C36NYR66",
  "angularObjects": {
    "2BY98S9W2:shared_process": [],
    "2BZN39DZF:shared_process": [],
    "2BY515SJU:shared_process": [],
    "2BY63HPYF:shared_process": [],
    "2BXGT6S2K:shared_process": [],
    "2BZ613KSU:shared_process": [],
    "2BZ8A7E2E:shared_process": [],
    "2BZSKZKCV:shared_process": [],
    "2BXPP47F9:shared_process": [],
    "2BXU3K7UZ:shared_process": [],
    "2BYUGR38A:shared_process": [],
    "2BYBYAWVT:shared_process": [],
    "2C1KUKSFC:shared_process": [],
    "2BXESRBUA:shared_process": [],
    "2BYNRTR44:shared_process": [],
    "2BY67DHM2:shared_process": [],
    "2BXUY475Z:shared_process": [],
    "2BZPA2E8W:shared_process": []
  },
  "config": {},
  "info": {}
}