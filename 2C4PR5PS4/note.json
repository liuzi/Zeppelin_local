{
  "paragraphs": [
    {
      "text": "import org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter._\nimport org.apache.spark.storage.StorageLevel\nimport scala.io.Source\nimport scala.collection.mutable.HashMap\nimport java.io.File\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport sys.process.stringSeqToProcess\n\n\nval conf \u003d new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\")\nval ssc \u003d new StreamingContext(conf, Seconds(1))\nval lines \u003d ssc.socketTextStream(\"localhost\",7777)\nval errorLines \u003d lines.filter(_.contains(\"error\"))\nerrorLines.print()",
      "dateUpdated": "Nov 24, 2016 4:26:17 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1479973639181_1553002751",
      "id": "20161124-154719_1715604258",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark._\n\nimport org.apache.spark.streaming._\n\nimport org.apache.spark.streaming.StreamingContext._\n\nconf: org.apache.spark.SparkConf \u003d org.apache.spark.SparkConf@cb9b375\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts \u003d true. The currently running SparkContext was created at:\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:823)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)\norg.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:33)\norg.apache.zeppelin.spark.SparkInterpreter.createSparkSession(SparkInterpreter.java:337)\norg.apache.zeppelin.spark.SparkInterpreter.getSparkSession(SparkInterpreter.java:217)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:816)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\norg.apache.zeppelin.scheduler.Job.run(Job.java:176)\norg.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\njava.util.concurrent.FutureTask.run(FutureTask.java:266)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2221)\n  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2217)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2217)\n  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2290)\n  at org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:89)\n  at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:836)\n  at org.apache.spark.streaming.StreamingContext.\u003cinit\u003e(StreamingContext.scala:84)\n  ... 53 elided\n"
      },
      "dateCreated": "Nov 24, 2016 3:47:19 AM",
      "dateStarted": "Nov 24, 2016 3:59:52 AM",
      "dateFinished": "Nov 24, 2016 4:00:15 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter._\nimport org.apache.spark.storage.StorageLevel\nimport scala.io.Source\nimport scala.collection.mutable.HashMap\nimport java.io.File\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport sys.process.stringSeqToProcess\n\nval ssc \u003d new StreamingContext(sc, Seconds(2))\nval tweets \u003d TwitterUtils.createStream(ssc, None)\nval twt \u003d tweets.window(Seconds(60))\n\ncase class Tweet(createdAt:Long, text:String)\ntwt.map(status\u003d\u003e\n  Tweet(status.getCreatedAt().getTime()/1000, status.getText())\n).foreachRDD(rdd\u003d\u003e\n  // Below line works only in spark 1.3.0.\n  // For spark 1.1.x and spark 1.2.x,\n  // use rdd.registerTempTable(\"tweets\") instead.\n  rdd.toDF().registerAsTable(\"tweets\")\n)\ntwt.print\n\nssc.start()",
      "dateUpdated": "Nov 24, 2016 4:30:15 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1479976009898_621524020",
      "id": "20161124-162649_880940859",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.streaming._\n\nimport org.apache.spark.streaming.twitter._\n\nimport org.apache.spark.storage.StorageLevel\n\nimport scala.io.Source\n\nimport scala.collection.mutable.HashMap\n\nimport java.io.File\n\nimport org.apache.log4j.Logger\n\nimport org.apache.log4j.Level\n\nimport sys.process.stringSeqToProcess\n\nssc: org.apache.spark.streaming.StreamingContext \u003d org.apache.spark.streaming.StreamingContext@51d0de58\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njava.lang.NoClassDefFoundError: org/apache/spark/Logging\n  at java.lang.ClassLoader.defineClass1(Native Method)\n  at java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n  at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n  at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n  at org.apache.spark.streaming.twitter.TwitterUtils$.createStream(TwitterUtils.scala:44)\n  ... 60 elided\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.Logging\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n  ... 72 more\n"
      },
      "dateCreated": "Nov 24, 2016 4:26:49 AM",
      "dateStarted": "Nov 24, 2016 4:30:15 AM",
      "dateFinished": "Nov 24, 2016 4:30:18 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql select * from tweets where text like \u0027%girl%\u0027 limit 10",
      "dateUpdated": "Nov 24, 2016 4:30:48 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1479976237312_-446632516",
      "id": "20161124-163037_1769216322",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Table or view not found: tweets; line 1 pos 14\nset zeppelin.spark.sql.stacktrace \u003d true to see full stacktrace"
      },
      "dateCreated": "Nov 24, 2016 4:30:37 AM",
      "dateStarted": "Nov 24, 2016 4:30:48 AM",
      "dateFinished": "Nov 24, 2016 4:31:00 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%dep\nz.load(\"org.apache.spark:spark-streaming-twitter_2.10:1.5.1\")",
      "dateUpdated": "Nov 24, 2016 4:24:52 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1479973992462_-1336742781",
      "id": "20161124-155312_379811417",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "DepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency \u003d org.apache.zeppelin.dep.Dependency@52fa08bf\n"
      },
      "dateCreated": "Nov 24, 2016 3:53:12 AM",
      "dateStarted": "Nov 24, 2016 4:24:53 AM",
      "dateFinished": "Nov 24, 2016 4:25:33 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// scalastyle:off println\n//package org.apache.spark.examples.mllib\n\nimport org.apache.spark.SparkConf\n// $example on$\nimport org.apache.spark.mllib.clustering.StreamingKMeans\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n// $example off$\n\n/**\n * Estimate clusters on one stream of data and make predictions\n * on another stream, where the data streams arrive as text files\n * into two different directories.\n *\n * The rows of the training text files must be vector data in the form\n * `[x1,x2,x3,...,xn]`\n * Where n is the number of dimensions.\n *\n * The rows of the test text files must be labeled data in the form\n * `(y,[x1,x2,x3,...,xn])`\n * Where y is some identifier. n must be the same for train and test.\n *\n * Usage:\n *   StreamingKMeansExample \u003ctrainingDir\u003e \u003ctestDir\u003e \u003cbatchDuration\u003e \u003cnumClusters\u003e \u003cnumDimensions\u003e\n *\n * To run on your local machine using the two directories `trainingDir` and `testDir`,\n * with updates every 5 seconds, 2 dimensions per data point, and 3 clusters, call:\n *    $ bin/run-example mllib.StreamingKMeansExample trainingDir testDir 5 3 2\n *\n * As you add text files to `trainingDir` the clusters will continuously update.\n * Anytime you add text files to `testDir`, you\u0027ll see predicted labels using the current model.\n *\n */\n//object StreamingKMeansExample {\n\n // def main(args: Array[String]) {\n//    if (args.length !\u003d 5) {\n//      System.err.println(\n//        \"Usage: StreamingKMeansExample \" +\n//          \"\u003ctrainingDir\u003e \u003ctestDir\u003e \u003cbatchDuration\u003e \u003cnumClusters\u003e \u003cnumDimensions\u003e\")\n//      System.exit(1)\n//    }\n\n    // $example on$\n//    val conf \u003d new SparkConf().setAppName(\"StreamingKMeansExample\")\n    val ssc \u003d new StreamingContext(sc, Seconds(2))\n\n    val trainingData \u003d ssc.textFileStream(args(0)).map(Vectors.parse)\n    val testData \u003d ssc.textFileStream(args(1)).map(LabeledPoint.parse)\n\n    val model \u003d new StreamingKMeans()\n      .setK(args(3).toInt)\n      .setDecayFactor(1.0)\n      .setRandomCenters(args(4).toInt, 0.0)\n\n    model.trainOn(trainingData)\n    model.predictOnValues(testData.map(lp \u003d\u003e (lp.label, lp.features))).print()\n\n    ssc.start()\n    ssc.awaitTermination()\n    // $example off$\n//  }\n//}\n// scalastyle:on println\n",
      "dateUpdated": "Dec 2, 2016 10:49:50 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1479975892978_80218423",
      "id": "20161124-162452_1565892369",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.SparkConf\n\nimport org.apache.spark.mllib.clustering.StreamingKMeans\n\nimport org.apache.spark.mllib.linalg.Vectors\n\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\nconf: org.apache.spark.SparkConf \u003d org.apache.spark.SparkConf@477b13e7\n\n\n\n\u003cconsole\u003e:42: error: not found: value args\n           val ssc \u003d new StreamingContext(conf, Seconds(args(2).toLong))\n                                                        ^\n"
      },
      "dateCreated": "Nov 24, 2016 4:24:52 AM",
      "dateStarted": "Dec 2, 2016 10:48:48 AM",
      "dateFinished": "Dec 2, 2016 10:48:50 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nval conf \u003d new SparkConf().setMaster(\"local\").setAppName(\"My APP\")",
      "dateUpdated": "Dec 2, 2016 11:00:25 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1480646066205_-1342745056",
      "id": "20161202-103426_686776014",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark.SparkConf\n\nimport org.apache.spark.SparkContext\n\nimport org.apache.spark.SparkContext._\n\nconf: org.apache.spark.SparkConf \u003d org.apache.spark.SparkConf@62579eb\n"
      },
      "dateCreated": "Dec 2, 2016 10:34:26 AM",
      "dateStarted": "Dec 2, 2016 11:00:25 AM",
      "dateFinished": "Dec 2, 2016 11:00:26 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val ssc \u003d new SparkContext(conf)",
      "dateUpdated": "Dec 2, 2016 11:00:47 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1480646168122_-1432822271",
      "id": "20161202-103608_1334924555",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts \u003d true. The currently running SparkContext was created at:\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:823)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)\norg.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:33)\norg.apache.zeppelin.spark.SparkInterpreter.createSparkSession(SparkInterpreter.java:337)\norg.apache.zeppelin.spark.SparkInterpreter.getSparkSession(SparkInterpreter.java:217)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:816)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\norg.apache.zeppelin.scheduler.Job.run(Job.java:176)\norg.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\njava.util.concurrent.FutureTask.run(FutureTask.java:266)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\njava.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2221)\n  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2217)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2217)\n  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2290)\n  at org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:89)\n  ... 48 elided\n"
      },
      "dateCreated": "Dec 2, 2016 10:36:08 AM",
      "dateStarted": "Dec 2, 2016 11:00:47 AM",
      "dateFinished": "Dec 2, 2016 11:00:48 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Dec 2, 2016 11:00:57 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1480647647614_-281158255",
      "id": "20161202-110047_1579935953",
      "dateCreated": "Dec 2, 2016 11:00:47 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/Learning/sparkStreaming",
  "id": "2C4PR5PS4",
  "angularObjects": {
    "2BY98S9W2:shared_process": [],
    "2BZN39DZF:shared_process": [],
    "2BY515SJU:shared_process": [],
    "2BY63HPYF:shared_process": [],
    "2BXGT6S2K:shared_process": [],
    "2BZ613KSU:shared_process": [],
    "2BZ8A7E2E:shared_process": [],
    "2BZSKZKCV:shared_process": [],
    "2BXPP47F9:shared_process": [],
    "2BXU3K7UZ:shared_process": [],
    "2BYUGR38A:shared_process": [],
    "2BYBYAWVT:shared_process": [],
    "2C1KUKSFC:shared_process": [],
    "2BXESRBUA:shared_process": [],
    "2BYNRTR44:shared_process": [],
    "2BY67DHM2:shared_process": [],
    "2BXUY475Z:shared_process": [],
    "2BZPA2E8W:shared_process": []
  },
  "config": {},
  "info": {}
}